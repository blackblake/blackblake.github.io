# 第1章：简介

## **1.1 强化学习**

---

### （一）定义

强化学习是一种学习如何将状态映射到动作，以获得最大奖励的学习机制；

学习者不会被告知要采取哪些action，而是必须通过尝试来发现哪些action会产生最大的reward；

action不仅可以影响直接奖励，还可以影响下一个状态，并通过下一个状态，影响到随后而来的奖励。 ；

### （二）3种意义

reinforcement learning这个名词同时代表了强化学习的“问题，方法，领域”这三者，在强化学习中，区分问题和解决问题的方法是非常重要的；没有做出这种区分是许多混乱的根源。

### （三）强化学习的方法

对实际问题最重要的方面进行采样，训练个体，使其多次与环境交互去达成一个目标。 个体必须能够**感知**其环境状态，并且能够采取**动作**(action)影响环境的状态。 个体还必须具有与环境状态相关的一个或多个**目标**。 

### （四）强化学习的特殊之处

- 强化学习不同于 *监督学习* ，后者从监管者(supervisor)所提供的有标签的训练集中学习。每一个**样例(example)**都是一种情况的描述，都带有**标签(label)**，标签描述了系统在该情况下的应该采取的正确动作，每一个样例用来识别这种情况应该属于哪一类。
- 强化学习也与 *无监督学习* 不同，后者通常用于寻找隐藏在无标签的数据集中的结构。

- 强化学习的一个特殊之处，是如何**权衡探索（Exploration）与利用（Exploitation）之间的关系**：为了获得大量奖励，强化学习个体必须倾向于过去已经尝试过并且能够有效获益的行动。 但是要发现这样的行为，它必须尝试以前没有选择过的行为。 个体必须充分 *利用* 它既有经验以获得收益，但它也必须 *探索*，以便在未来做出更好的动作选择。
- 强化学习的另一个关键特征是：它明确地考虑了目标导向的智能体与不确定的环境交互这**整个问题(whole problem)**。而很多其他方法都是只考虑子问题，而忽视了子问题在更大情境下的适用性。
    
    所有强化学习的智能体都有一个明确的目标，即能够感知环境的各个方面，并可以选择动作来影响它们所处的环境。
    
- 我们所提到的完整的、交互式的、目标导向的智能体并不一定是完整的有机体，它们也**可以是一个更大的系统的组成部分**。在这种情况下，智能体**直接**与这个系统的其他部分进行交互，并**间接**地与系统的环境交互。

## 1.2 示例

---

- 智能体的**动作会影响未来环境的状态**(例如，国际象棋落子的位置、炼油厂的水位线，以及机器人的下一个位置和电量),进而影响未来的决策和机会。因此正确的选择需要考虑到间接的、延迟的动作后果。
- 与此同时，在所有这些例子中，我们**无法完全预测动作的影响**，因此智能体**必须频繁地监视其环境**并做出适当的反应。例如，菲尔必须看着牛奶倒人麦片碗以防溢出。
- 所有这些例子都涉及**明确的目标**，智能体可以根据这个目标来判断进展。例如国际象棋选手知道他是否得胜，炼油厂的控制器知道石油产量，菲尔知道他是否在享受他的早餐。

## 1.3 强化学习要素

---

1）**策略：**定义了学习智能体在特定时间的行为方式，是环境状态到动作的映射。在某些情况下，策略可能是一个简单的函数或查询表，而在另一些情况下，它可能涉及大量的计算，例如搜索过程。

2）**收益(return)：**定义了“目标”。在每一步中，环境向强化学习智能体发送一个称为收益的标量数值。智能体的唯一目标是最大化长期总收益。

3）**价值函数(value)：**收益信号表明了在短时间内什么是好的，而价值函数则表示了从长远的角度看什么是好的。简单地说，一个状态的价值是一个智能体从这个状态开始，对将来累积的总收益的期望。尽管收益决定了环境状态直接、即时、内在的吸引力，但价值表示了接下来所有可能状态的长期期望。

> 从某种意义上来说，收益更加重要，而作为收益预测的价值次之。没有收益就没有价值，而评估价值的唯一目的就是获得更多的收益。然而在制定和评估策略时，我们最关心的是价值。
> 

确定价值要比确定收益难得多。收益基本上是由环境直接给予的，但是价值必须综合评估，并根据智能体在整个过程中观察到的收益序列重新估计。事实上，**价值评估方法才是几乎所有强化学习算法中最重要的组成部分**。

4）**对环境建立的模型：**即对环境的反应模式的模拟，或者更一般地说，它允许对外部环境的行为进行推断。例如，给定一个状态和动作，模型就可以预测外部环境的下一个状态和下一个收益。

环境模型会被用于做规划。规划，就是在真正经历之前，先考虑未来可能发生的各种情境从而预先决定采取何种动作，而简单的无模型的方法则是直接地试错。

## 1.4 本身的局限性

---

- 本书不处理构建、改变或学习**状态信号**的问题，而专注于策略问题。换句话说，本书的关注点不在于设计状态信号，而在于根据给定状态信号来决定采取什么动作。
- 本书中讨论的大多数强化学习方法建立在对**价值函数**的估计上。但是这并不是解决强化学习问题的必由之路。

## 1.5 强化学习示例—井字棋

---

这个例子没怎么看懂，涉及到**时序差分学习**（第6章），等学完这一章后再回顾。
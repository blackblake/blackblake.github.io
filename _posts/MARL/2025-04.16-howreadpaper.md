---
title: How to Read Papers
date: 2025-04-16 03:39:16 +0800
categories:
    - Multi-Agent Reinforcement Learning
tags:
    - marl
lastmod: 2025-04-16T11:12:31.385Z
---
[by HeartofMachine](https://www.jiqizhixin.com/articles/2019-02-22-5)

You should accept that you won't understand the contents of most papers. At your level reading papers is mostly about understanding the structure of research papers, identifying and understanding their goals, and understanding whether and why their final results show that they have accomplished their goals. If you can do all of that then you're in a very good place.

Also know that part of the difficulty of understanding papers is that a lot of them are genuinely not good. This is especially true in machine learning. 
The logical justification for research methods is often hand wavy or even just wrong, and most authors aren't good at explaining what they've done. Results are frequently contextualized poorly, incorrectly, or misleadingly. A lot of published papers (probably most) are mostly pointless.

1. **Take multiple passes through the paper**
    - Worst strategy: reading from the first word until the last word!
2. **Read the Title/Abstract/Figures**
    - Especially in deep learning, there are a lot of papers where the entire paper is summarized in one or two figures.
    - You can often get a good understanding about what the whole paper is about without reading much of the text.
3. **Read the Introduction/Conclusions/Figures (again)/Skim Rest**
    - Part of the process of writing papers is convincing the reviewers that your paper is worthy of acceptance, so you find that the abstract, intro, and conclusion are where the authors summarize their work really carefully. These are therefore the most useful parts to read.
    - Neural network architectures are often written up in a table.
    - Maybe also skim *Related work* section, for context or see if there is something you have read before.
4. **Read the Paper, but skip the maths**
5. **Read the Paper, but skip parts that don’t make sense**
    - In cutting edge papers we don’t always know what is really important and what isn’t important.
    - Some great, highly cited papers have some parts which are groundbreaking and other parts which later turn out to be unimportant, but at the time the paper was written the authors could not know.
    - Maybe what was the key part of the algorithm wasn’t what the authors thought.
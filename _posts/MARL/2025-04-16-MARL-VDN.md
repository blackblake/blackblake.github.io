---
title: VDN
date: 2025-04-16 03:39:16 +0800
categories:
- 多智能体强化学习
tags:
- marl
math: true
---

[知乎链接](https://zhuanlan.zhihu.com/p/362191316)

VDN的核心思想
---
**将联合动作值函数 $Q_{tot}$ 分解为多个子智能体的局部值函数 $Q_i$ 的和**，即：
$Q_{tot}(\mathbf{s}, \mathbf{a}) = \sum_{i=1}^{N} Q_i(s_i, a_i)$
其中：

- $\mathbf{s} = (s_1, ..., s_N)$ 是联合状态
- $\mathbf{a} = (a_1, ..., a_N)$ 是联合动作
- $Q_i(s_i, a_i)$ 是第 $i$ 个子智能体的局部值函数

### **问题1：VDN 的值函数分解是否保证子函数相加等于总体值函数？**

**是的**，VDN 的分解方式是 **严格的线性求和**，即：
$Q_{tot}(\mathbf{s}, \mathbf{a}) = Q_1(s_1, a_1) + Q_2(s_2, a_2) + \cdots + Q_N(s_N, a_N)$
这意味着：

- **全局最优动作** 等价于 **所有子智能体局部最优动作的组合**（因为最大化 $Q_{tot}$ 等价于分别最大化每个 $Q_i$）。
- **训练时**，VDN 使用 TD error **反向传播更新所有 $Q_i$**，确保它们的和逼近真实的 $Q_{tot}$。



### **问题2：如何设计子函数 $Q_i$？**

子函数 $Q_i$ 的设计需要满足 **可分解性** 和 **可训练性**，具体方法如下：

### **(1) 网络结构设计**

- **输入**：每个 $Q_i$ 仅接收 **局部观测 $s_i$** 和 **自身动作 $a_i$**（不直接访问其他智能体的信息）。
- **输出**：标量值 $Q_i(s_i, a_i)$，表示当前状态下采取动作 $a_i$ 的预期回报。
- **共享参数**（可选）：如果子智能体是同质的（homogeneous），可以共享 $Q_i$ 的网络参数以减少计算量。

### **(2) 训练流程**

1. **集中式训练（Centralized Training）**：
    - 在训练时，收集所有子智能体的 $Q_i$，计算它们的和 $Q_{tot}$。
    - 用 **全局 TD error** 更新所有 $Q_i$：
    $\mathcal{L} = \left( r + \gamma \max_{\mathbf{a}'} Q_{tot}(\mathbf{s}', \mathbf{a}') - Q_{tot}(\mathbf{s}, \mathbf{a}) \right)^2$
    - 反向传播时，梯度会分配到各个 $Q_i$。
2. **分布式执行（Decentralized Execution）**：
    - 执行时，每个子智能体只需基于自己的 $Q_i$ 选择动作：
    $a_i = \arg\max_{a_i} Q_i(s_i, a_i)$
    - 无需通信或全局信息。


### **问题3：VDN 的局限性及改进方向**

- **局限性**：
    - **线性分解过于严格**：现实任务中，子智能体的协作可能是非线性的（如某些动作组合优于单独动作的和）。
    - **部分可观测问题**：如果 $Q_i$ 仅依赖局部观测，可能无法学到全局最优策略。
- **改进方法**：
    - **QMIX**：引入非线性混合网络（输入全局状态 $\mathbf{s}$），保证单调性：$\frac{\partial Q_{tot}}{\partial Q_i} \geq 0$。
    - **QTRAN**：放松单调性约束，通过额外约束保证分解的合理性。



VDN中的网络
---
在 **VDN（Value Decomposition Networks）** 中，“网络(Network)”主要体现在 **子智能体的局部值函数 $Q_i$ 由神经网络参数化**，并通过深度学习的训练方式联合优化。具体来说，其“网络”特性体现在以下方面：


### **1. 子智能体的 $Q_i$ 由神经网络实现**
每个子智能体的局部值函数 $Q_i(s_i, a_i)$ 是一个独立的神经网络（通常是全连接网络或卷积网络）
- **输入**：子智能体的局部观测 $s_i$（不依赖其他智能体的信息）。
- **输出**：该智能体在所有可能动作 $a_i$ 上的 Q 值。



### **2. 联合训练：通过全局 $Q_{tot}$ 反向传播**
VDN 的核心是通过 **集中式训练（Centralized Training）** 来更新所有子网络的参数：
1. **前向计算**：所有子网络的输出 $Q_i$ 相加得到全局 $Q_{tot}$：
$$Q_{tot}(\mathbf{s}, \mathbf{a}) = \sum_{i=1}^N Q_i(s_i, a_i)$$
2. **损失函数**：使用全局 TD error 计算损失：
  $$
   \mathcal{L} = \left( r + \gamma \max_{\mathbf{a}'} Q_{tot}(\mathbf{s}', \mathbf{a}') - Q_{tot}(\mathbf{s}, \mathbf{a}) \right)^2
  $$
3. **反向传播**：梯度通过 $Q_{tot}$ 分配到各个子网络 $Q_i$，更新它们的参数。


### **3. 为什么需要“网络”？**
1. **函数逼近能力**：  
   - 神经网络可以拟合复杂的 $Q_i$ 函数，解决高维状态空间问题（如视觉输入）。
2. **端到端训练**：  
   - 通过反向传播自动学习智能体之间的协作关系，无需手工设计规则。
3. **泛化性**：  
   - 网络可以泛化到未见过的状态组合，适应动态环境。



子函数划分、VDN与IQL的区别
---
### 子函数划分的实现方式
1. **初始化阶段**：  
   - 每个智能体的子函数 $Q_i$ 通常以随机值初始化（而非无意义的值），但会满足VDN的加和约束。
   - 加和约束：由于VDN的分解形式是加性的，子函数的初始化需保证联合 $Q_{tot}$ 的初始值合理（例如，若 $Q_{tot}$ 的合理范围是 $[0, 100]$，则每个 $Q_i$ 的初始值应避免极端偏离）。
   - 初始时智能体的策略是随机的或基于简单启发式规则。

2. **学习过程**：  
   - 通过全局奖励信号反向传播，优化各 $Q_i$ 的参数。梯度从 $Q_{tot}$ 分配到各子函数，使得每个 $Q_i$ 逐渐学习到局部观察 $s_i$ 和动作 $a_i$ 对全局奖励的贡献。
   - 智能体通过探索（如 $\epsilon$-greedy）发现高收益策略，并利用联合价值函数的分解结构将协作行为编码到子函数中。

### 与IQL（Independent Q-Learning）的区别

1. **结构化约束**：  
   VDN的加和分解强制智能体考虑自身行为对全局值的影响，而IQL的独立更新无法保证这一点。例如，在合作捕猎任务中，VDN会通过 $Q_{tot}$ 的梯度调整各 $Q_i$，使智能体学会包围猎物；而IQL可能因忽略其他智能体动作导致策略冲突。

2. **非平稳性问题**：  
   IQL中每个智能体将其他智能体视为环境的一部分，其策略变化会导致环境动态不稳定（即 $P(s' \mid s, a_i)$ 受其他智能体影响）。VDN通过联合优化缓解此问题。

3. **探索效率**：  
   VDN的全局视角能更高效地分配探索信用。例如，若某次高奖励源于智能体A和B的 **协同动作**，VDN会 **同时更新两者的 $Q_i$**，而IQL可能仅A或B偶然学习到部分贡献。
---
title: VDN
date: 2025-04-16 03:39:16 +0800
categories:
    - Multi-Agent Reinforcement Learning
    - Algorithms
tags:
    - marl
lastmod: 2025-04-16T11:12:14.475Z
---
[知乎链接](https://zhuanlan.zhihu.com/p/362191316)

VDN的核心思想
---
**将联合动作值函数 $Q_{tot}$ 分解为多个子智能体的局部值函数 $Q_i$ 的和**，即：
$Q_{tot}(\mathbf{s}, \mathbf{a}) = \sum_{i=1}^{N} Q_i(s_i, a_i)$
其中：

- $\mathbf{s} = (s_1, ..., s_N)$ 是联合状态
- $\mathbf{a} = (a_1, ..., a_N)$ 是联合动作
- $Q_i(s_i, a_i)$ 是第 $i$ 个子智能体的局部值函数

### **问题1：VDN 的值函数分解是否保证子函数相加等于总体值函数？**

**是的**，VDN 的分解方式是 **严格的线性求和**，即：
$Q_{tot}(\mathbf{s}, \mathbf{a}) = Q_1(s_1, a_1) + Q_2(s_2, a_2) + \cdots + Q_N(s_N, a_N)$
这意味着：

- **全局最优动作** 等价于 **所有子智能体局部最优动作的组合**（因为最大化 $Q_{tot}$ 等价于分别最大化每个 $Q_i$）。
- **训练时**，VDN 使用 TD error **反向传播更新所有 $Q_i$**，确保它们的和逼近真实的 $Q_{tot}$。



### **问题2：如何设计子函数 $Q_i$？**

子函数 $Q_i$ 的设计需要满足 **可分解性** 和 **可训练性**，具体方法如下：

### **(1) 网络结构设计**

- **输入**：每个 $Q_i$ 仅接收 **局部观测 $s_i$** 和 **自身动作 $a_i$**（不直接访问其他智能体的信息）。
- **输出**：标量值 $Q_i(s_i, a_i)$，表示当前状态下采取动作 $a_i$ 的预期回报。
- **共享参数**（可选）：如果子智能体是同质的（homogeneous），可以共享 $Q_i$ 的网络参数以减少计算量。

### **(2) 训练流程**

1. **集中式训练（Centralized Training）**：
    - 在训练时，收集所有子智能体的 $Q_i$，计算它们的和 $Q_{tot}$。
    - 用 **全局 TD error** 更新所有 $Q_i$：
    $\mathcal{L} = \left( r + \gamma \max_{\mathbf{a}'} Q_{tot}(\mathbf{s}', \mathbf{a}') - Q_{tot}(\mathbf{s}, \mathbf{a}) \right)^2$
    - 反向传播时，梯度会分配到各个 $Q_i$。
2. **分布式执行（Decentralized Execution）**：
    - 执行时，每个子智能体只需基于自己的 $Q_i$ 选择动作：
    $a_i = \arg\max_{a_i} Q_i(s_i, a_i)$
    - 无需通信或全局信息。


### **问题3：VDN 的局限性及改进方向**

- **局限性**：
    - **线性分解过于严格**：现实任务中，子智能体的协作可能是非线性的（如某些动作组合优于单独动作的和）。
    - **部分可观测问题**：如果 $Q_i$ 仅依赖局部观测，可能无法学到全局最优策略。
- **改进方法**：
    - **QMIX**：引入非线性混合网络（输入全局状态 $\mathbf{s}$），保证单调性：$\frac{\partial Q_{tot}}{\partial Q_i} \geq 0$。
    - **QTRAN**：放松单调性约束，通过额外约束保证分解的合理性。



VDN中的网络
---
在 **VDN（Value Decomposition Networks）** 中，“网络(Network)”主要体现在 **子智能体的局部值函数 $Q_i$ 由神经网络参数化**，并通过深度学习的训练方式联合优化。具体来说，其“网络”特性体现在以下方面：


### **1. 子智能体的 $Q_i$ 由神经网络实现**
每个子智能体的局部值函数 $Q_i(s_i, a_i)$ 是一个独立的神经网络（通常是全连接网络或卷积网络）
- **输入**：子智能体的局部观测 $s_i$（不依赖其他智能体的信息）。
- **输出**：该智能体在所有可能动作 $a_i$ 上的 Q 值。



### **2. 联合训练：通过全局 $Q_{tot}$ 反向传播**
VDN 的核心是通过 **集中式训练（Centralized Training）** 来更新所有子网络的参数：
1. **前向计算**：所有子网络的输出 $Q_i$ 相加得到全局 $Q_{tot}$：
$$Q_{tot}(\mathbf{s}, \mathbf{a}) = \sum_{i=1}^N Q_i(s_i, a_i)$$
2. **损失函数**：使用全局 TD error 计算损失：
  $$
   \mathcal{L} = \left( r + \gamma \max_{\mathbf{a}'} Q_{tot}(\mathbf{s}', \mathbf{a}') - Q_{tot}(\mathbf{s}, \mathbf{a}) \right)^2
  $$
3. **反向传播**：梯度通过 $Q_{tot}$ 分配到各个子网络 $Q_i$，更新它们的参数。


### **3. 为什么需要“网络”？**
1. **函数逼近能力**：  
   - 神经网络可以拟合复杂的 $Q_i$ 函数，解决高维状态空间问题（如视觉输入）。
2. **端到端训练**：  
   - 通过反向传播自动学习智能体之间的协作关系，无需手工设计规则。
3. **泛化性**：  
   - 网络可以泛化到未见过的状态组合，适应动态环境。


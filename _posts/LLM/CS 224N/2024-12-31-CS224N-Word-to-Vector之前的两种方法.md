---
title: Word to Vector：之前的两种方法
date: 2024-12-31 11:12:01 +0800
categories:
- LLM
- CS224N
tags:
- algorithm
- llm
math: true
---

## Word to Vector：之前的两种方法
---
我们来详细解释一下这两种方法。

这两类方法的目标都是一样的：把词语转换成计算机可以理解和处理的数字向量（即“词向量”），同时让意思相近的词在向量空间中的位置也相近。

### 1. 基于计数的方法 (Count-based Methods)

**核心思想**：一个词的含义是由它周边的词来决定的。如果两个词经常出现在相似的上下文环境中（**一个“上下文”** 通常可以是一篇文章、一个段落，或者一个网页），那它们的词义就相近。

**典型代表**：LSA (Latent Semantic Analysis, 潜在语义分析)

**工作流程**：

1. **构建共现矩阵 (Co-occurrence Matrix)**：首先，算法会扫描海量的文本数据（比如全部维基百科文章）。然后，它会创建一个巨大的矩阵。这个矩阵的行代表词汇表里的每一个词，列也代表词汇表里的每一个词（或者代表一个文档、一个段落）。矩阵中的每一个数字，记录的是对应的两个词在指定大小的“上下文窗口”内共同出现了多少次。
    
    - 例如，有一个句子“我喜欢吃苹果，也喜欢吃香蕉”。如果窗口大小是2，那么“喜欢”和“吃”、“吃”和“苹果”、“吃”和“香蕉”的共现次数就会被记录下来。
        
2. **降维 (Dimensionality Reduction)**：这个共现矩阵通常非常巨大且稀疏（大部分是0）。直接使用它作为词向量效果不好。因此，需要使用一种叫做**奇异值分解 (SVD)** 的数学技术来对这个大矩阵进行“降维”。SVD 可以抓住矩阵中最重要的模式，把高维的稀疏矩阵压缩成一个低维度的、密集的矩阵。
    
3. **生成词向量**：降维之后得到的矩阵，每一行就对应着一个词的向量表示。
    


**总结**：

- **优点**：能从整体上把握全局的统计信息，因为它是基于整个语料库来计数的。
    
- **缺点**：对于词语之间的线性关系（如词义类比）捕捉得不好。此外，对词频高的词（如“的”、“是”）过于敏感。
    

---

### 2. 基于预测/窗口的方法 (Shallow Window-based Methods)

**核心思想**：通过一个简单的神经网络，让它在阅读文本时不断进行“预测任务”，在完成任务的过程中，模型就“学会”了每个词的向量表示。

**典型代表**：Word2Vec (包含 CBOW 和 Skip-gram 两种模型)

工作流程：

这种方法不直接统计全局的共现次数，而是使用一个滑动的“上下文窗口”来一段一段地学习。

- **CBOW (Continuous Bag-of-Words, 连续词袋模型)**
    
    1. **任务**：根据一个词的上下文（周围的词）来**预测这个词本身**。
        
    2. **例子**：对于句子 `"... a cat sitting on the mat"`，如果目标词是 `sitting`，CBOW 模型会利用上下文 `[a, cat, on, the]` 来预测中间的词是 `sitting`。
        
    3. **学习过程**：模型输入是上下文词语的向量，输出是对目标词的预测。通过不断调整网络参数，使得预测越来越准。训练结束后，网络内部的权重就构成了我们想要的词向量。
        
- **Skip-gram**
    
    1. **任务**：和 CBOW 相反，它是根据一个词来**预测它周围的上下文**。
        
    2. **例子**：还是上面的句子，如果输入词是 `sitting`，Skip-gram 模型需要能预测出它周围的词是 `a`, `cat`, `on`, `the`。
        
    3. **学习过程**：通过一个词去预测多个周围的词。这个过程让模型学习到词语之间的搭配关系。同样，训练完成后，网络内部的权重就是词向量。
        

**总结**：

- **优点**：通过预测任务，能更好地捕捉词语之间复杂的语义关系，尤其在词义类比任务上表现出色。
    
- **缺点**：由于它只关注局部的上下文窗口，所以可能无法充分利用全局的统计信息。
    

## GloVe Model
---
### 1. 为什么需要“昂贵的求和计算”？

首先，回顾一下 skip-gram 模型使用的交叉熵损失函数，它存在一个“昂贵的求和计算”，这来自于 **Softmax 函数**，它在像 Skip-gram 这样的模型中被用来将一个数值（score）转换成概率分布。

我们来看一下 Softmax 公式：

$$Q_{ij}=\frac{exp(\vec{u}_{j}^{T}\vec{v}_{i})}{\sum_{w=1}^{N}exp(\vec{u}_{w}^{T}\vec{v}_{i})}$$

* **分子 $exp(\vec{u}_{j}^{T}\vec{v}_{i})$**:
    * $\vec{v}_{i}$ 是中心词 *i* 的词向量。
    * $\vec{u}_{j}$ 是上下文词 *j* 的词向量。
    * 它们的点积 $\vec{u}_{j}^{T}\vec{v}_{i}$ 计算出这两个词的“相似度”或“关联分数”。分数越高，代表模型认为它们越可能一起出现。
    * 取指数 `exp()` 是为了将分数变为正数。

* **分母 $\sum_{w=1}^{N}exp(\vec{u}_{w}^{T}\vec{v}_{i})$**:
    * **这就是“昂贵的求和”所在**。为了将分子的“关联分数”变成一个真正的概率（即所有可能情况的概率总和为1），我们必须计算中心词 *i* 与 **词汇表中所有词**（从第1个词到第 N 个词，N 是词汇表总大小）的关联分数，并将它们全部加起来作为分母。
    * **为什么昂贵？** 现代的词汇表（N）通常非常庞大，可能有几十万甚至上百万个词。在模型训练的每一步中，为了计算一个词的概率，都需要遍历整个词汇表并做点积和求和运算，这是一个巨大的计算负担。

因此，这个归一化步骤虽然在数学上是必需的（为了得到合法的概率分布），但在计算上是极其低效和昂贵的。

### 2. $\hat{P}$ 和 $\hat{Q}$ 之间的联系与平方差

GloVe 模型的设计初衷就是为了避开上面提到的昂贵的归一化步骤。它通过一个最小二乘目标 (Least Squares Objective) 来实现这一点。

#### $\hat{P}$ 和 $\hat{Q}$ 的定义与联系

* **$\hat{P}_{ij} = X_{ij}$**: 这里的 $X_{ij}$ 是词 *j* 出现在词 *i* 上下文中的 **实际共现次数**。你可以把它理解为从庞大语料库中统计出的“**事实**”或“**目标值**”。它直接反映了两个词在真实世界文本中的关联强度。

* **$\hat{Q}_{ij} = exp(\vec{u}_{j}^{T}\vec{v}_{i})$**: 这是由词向量 $\vec{u}_{j}$ 和 $\vec{v}_{i}$ 计算出的**模型预测值**。它代表模型根据当前学习到的词向量，所预测的两个词之间的关联强度。

**它们之间的联系是：** GloVe 模型的核心思想是，**让模型的预测值 ($\hat{Q}_{ij}$) 尽可能地去逼近语料库中的事实 ($\hat{P}_{ij}$)**。如果模型学得好，那么它基于词向量算出来的关联度，应该和真实文本里的共现次数所反映的关联度相匹配。

#### 为什么要计算它们的平方差？

计算平方差是**最小二乘法**的核心。其目标是让预测值和目标值之间的**平方误差 (squared error)** 最小化。

1. **初始目标**: GloVe 的初始想法是直接最小化 $(\hat{P}_{ij} - \hat{Q}_{ij})^2$，即 $(X_{ij} - exp(\vec{u}_{j}^{T}\vec{v}_{i}))^2$。这是一个非常直观的机器学习建模方式：定义一个目标，定义一个预测，然后让预测去拟合目标。

2. **问题与改进**: 但是，直接使用这个公式有一个问题：$X_{ij}$ 的取值范围非常大（从1到数百万），这会导致优化困难。因此，一个更有效的做法是去拟合它们**对数**。

3. **最终形式**: 模型的目标变成了最小化 **对数值的平方差**，即 $(log(\hat{P}_{ij}) - log(\hat{Q}_{ij}))^2$。将 $\hat{P}_{ij}$ 和 $\hat{Q}_{ij}$ 的定义代入，就得到：
    
    $$(log(X_{ij}) - log(exp(\vec{u}_{j}^{T}\vec{v}_{i})))^2 = (\vec{u}_{j}^{T}\vec{v}_{i} - log(X_{ij}))^2$$

这个改进后的形式出现在了最终的目标函数中。

总结来说，计算平方差是为了**构建一个损失函数**，通过最小化这个损失函数，来驱动模型不断调整词向量（$\vec{u}$ 和 $\vec{v}$），最终使得由词向量点积所反映的词间关系，能够与真实语料中的共现统计规律相一致。

### 3. 我的怀疑：

> 从表面上看，一个是从语料库中统计的**次数**，另一个是向量点积的**指数**，它们的量纲和数值范围都完全不同，直接将它们画等号似乎没有道理。

#### 详细解释

1. **定义角色：一个是目标，一个是预测**
    * **目标 (Target)**：共现次数 $X_{ij}$。它代表了从真实数据中观察到的、词 *i* 和词 *j* 之间的“真实关联强度”。这个数字是固定不变的，是模型需要学习和拟合的基准。
    * **预测 (Prediction)**：$exp(\vec{u}_{j}^{T}\vec{v}_{i})$。它代表了模型根据当前学到的词向量（$\vec{u}_j$ 和 $\vec{v}_i$）所预测的“关联强度”。这个值是会随着模型训练、词向量的更新而改变的。

    模型的目标就是：调整词向量，使得“预测”出来的关联强度去逼近“真实”的关联强度。

2. **认识到直接比较的缺陷**
    文件明确指出了直接使用最小二乘目标 $(\hat{P}_{ij} - \hat{Q}_{ij})^2$ 的问题，即 $X_{ij}$ 的值（也就是 $\hat{P}_{ij}$）经常会非常大，这使得优化变得困难。这说明模型设计者也知道，直接让 $exp(\vec{u}_{j}^{T}\vec{v}_{i})$ 去拟合一个可能高达数百万的 $X_{ij}$ 是不切实际且不稳定的。

3. **最终的解决方案：在对数空间中进行比较**
    为了解决上述问题，模型采取了一个关键步骤：**对目标值取对数 (logarithm)**。

    * 模型真正的目标不是让 $\vec{u}_{j}^{T}\vec{v}_{i}$ 去拟合 $X_{ij}$，而是让它去拟合 $log(X_{ij})$。
    * 我们来看最终目标函数的核心部分：$(\vec{u}_{j}^{T}\vec{v}_{i} - log(X_{ij}))^2$。
    * 这个公式清楚地表明，模型是在努力让向量的点积 $\vec{u}_{j}^{T}\vec{v}_{i}$ 的值，尽可能地接近共现次数的对数值 $log(X_{ij})$。

#### 为什么在对数空间中比较更合理？

* **压缩数值范围**：对数函数可以极大地压缩 $X_{ij}$ 的取值范围。一个从 10 变化到 1,000,000 的值，在取自然对数后，范围大约只在 2.3 到 13.8 之间。这使得数值更加稳定，模型更容易学习。
* **关注相对关系而非绝对值**：在对数空间中，模型不再关注共现次数的绝对差异，而是更关注它们的**数量级和比例关系**。例如，从 10 次到 100 次的差异（$log(100) - log(10) = 2.3$），和从 1000 次到 10000 次的差异（$log(10000) - log(1000) = 2.3$）在模型看来是等价的。这恰恰是语言中更有意义的统计规律——我们更关心“king”和“queen”一起出现的频率**相对于**其他词的频率，而不是它们共现的绝对次数。

因此，总结来说，$X_{ij}$ 和 $exp(\vec{u}_{j}^{T}\vec{v}_{i})$ 之所以能相提并论，是因为 GloVe 模型巧妙地将问题转化为了**在对数空间中，让向量点积 $\vec{u}_{j}^{T}\vec{v}_{i}$ 去拟合共现次数的对数 $log(X_{ij})$**。这不仅解决了数值不稳定的问题，也让模型学到了更有意义的词间相对关系。

